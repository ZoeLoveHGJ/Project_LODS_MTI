# -*- coding: utf-8 -*-
"""
ExpNew0_3.py
å®éªŒä¸‰ï¼šåˆ†ç»„å¤§å°ä¼˜åŒ–æµ‹è¯• (Group Size Optimization) - Optimized
ç›®æ ‡ï¼šåœ¨å¤§è§„æ¨¡æ ‡ç­¾åœºæ™¯ä¸‹ï¼Œå¯»æ‰¾ååé‡æœ€é«˜çš„ Group Size (K å€¼)ã€‚
"""

import logging
import random
import os
import concurrent.futures
import multiprocessing
import pandas as pd
from typing import Dict, Any

# --- å¯¼å…¥æ ¸å¿ƒç»„ä»¶ ---
from framework import (
    run_high_fidelity_simulation, 
    SimulationConfig, 
    Tag
)
from Tool import SimulationAnalytics
from lods_mti_algo import LODS_MTI_Algorithm

# æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger("ExpNew0_3")

# =========================================================
# ğŸ§ª å®éªŒé…ç½®
# =========================================================
TAG_COUNT = 500  # æµ·é‡æ ‡ç­¾ï¼Œæ”¾å¤§åè®®å¼€é”€
GROUP_SIZES = [4, 8, 12, 16, 24, 32, 48, 64,128,256]
REPEAT = 20       # é‡å¤æ¬¡æ•°ï¼Œå–å¹³å‡å€¼æ¶ˆé™¤æŠ–åŠ¨
OUTPUT_DIR = "Results_ExpNew0_3"
MAX_WORKERS = max(1, os.cpu_count() - 2)

def run_task(task_params: Dict[str, Any]) -> Dict[str, Any]:
    """å•ä¸ªå®éªŒä»»åŠ¡"""
    k = task_params['k']
    run_id = task_params['run_id']
    
    # 1. ç”Ÿæˆåœºæ™¯
    tags = [Tag(format(0xE2000000 + i, '024X')) for i in range(TAG_COUNT)]
    rng = random.Random(run_id)
    rng.shuffle(tags) # éšæœºåŒ–
    
    # 2. åˆå§‹åŒ–ç®—æ³•
    # å˜é‡: max_group_size = k
    # å›ºå®š: is_adaptive=False (æ’é™¤å¹²æ‰°), target_rho=4 (ä¿æŒç¨³å¥)
    algo = LODS_MTI_Algorithm(max_group_size=k, is_adaptive=True, target_rho=4)
    algo.initialize(tags)
    
    # 3. é…ç½®ç¯å¢ƒ (ç†æƒ³ç¯å¢ƒï¼Œä¸“æ³¨è€ƒå¯Ÿè°ƒåº¦æ•ˆç‡)
    cfg = SimulationConfig(
        TOTAL_TAGS=TAG_COUNT,
        ENABLE_NOISE=False,
        packet_error_rate=0.0,
        BIT_ERROR_RATE=0.0
    )
    
    # 4. è¿è¡Œä»¿çœŸ
    stats = run_high_fidelity_simulation(algo, cfg, tags)
    
    # 5. è®¡ç®—ååé‡
    total_time_s = stats['total_time_us'] / 1e6
    throughput = TAG_COUNT / total_time_s if total_time_s > 0 else 0
    
    # æ³¨å…¥è‡ªå®šä¹‰æŒ‡æ ‡
    stats['Throughput'] = throughput
    stats['Time_s'] = total_time_s
    
    return {
        "status": "success",
        "stats": stats,
        "sim_config": {"GroupSize": k}, # Xè½´
        "algorithm_name": "Improve_One", # å•ä¸€ç®—æ³•å¯¹æ¯”ä¸åŒå‚æ•°
        "run_id": run_id
    }

if __name__ == "__main__":
    multiprocessing.freeze_support()
    
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        print(f"ğŸ“‚ å·²è‡ªåŠ¨åˆ›å»ºè¾“å‡ºç›®å½•: {OUTPUT_DIR}")
        
    print(f"ğŸš€ å¯åŠ¨ Exp0_3: åˆ†ç»„å¤§å°ä¼˜åŒ– (Tags={TAG_COUNT})")
    print(f"âš™ï¸  é…ç½®: Workers={MAX_WORKERS}, GroupSizes={GROUP_SIZES}")
    
    analytics = SimulationAnalytics()
    
    # 1. æ„å»ºä»»åŠ¡
    tasks = []
    for k in GROUP_SIZES:
        for r in range(REPEAT):
            tasks.append({'k': k, 'run_id': r})
            
    total_tasks = len(tasks)
    print(f"ğŸ“‹ æ€»ä»»åŠ¡æ•°: {total_tasks} (æ­£åœ¨åˆ†å‘...)")
    
    # 2. å¹¶è¡Œæ‰§è¡Œ
    results_collected = 0
    with concurrent.futures.ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(run_task, t) for t in tasks]
        
        for future in concurrent.futures.as_completed(futures):
            try:
                res = future.result()
                results_collected += 1
                
                analytics.add_run_result(
                    result_stats=res['stats'],
                    sim_config=res['sim_config'],
                    algo_name=res['algorithm_name'],
                    run_id=res['run_id']
                )
                
                if results_collected % 10 == 0:
                    print(f"\rè¿›åº¦: {results_collected}/{total_tasks} ({(results_collected/total_tasks):.1%})", end="")
                    
            except Exception as e:
                logger.error(f"âŒ ä»»åŠ¡å¤±è´¥: {e}")
                
    print("\nâœ… æ‰€æœ‰ä»¿çœŸä»»åŠ¡å®Œæˆã€‚æ­£åœ¨ä¿å­˜æ•°æ®...")
    
    # 3. ä¿å­˜æ•°æ®
    # Xè½´ä¸º GroupSizeï¼Œç”Ÿæˆ raw_Throughput.csv, raw_Time_s.csv
    analytics.save_to_csv(x_axis_key='GroupSize', output_dir=OUTPUT_DIR)
    
    print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜è‡³: {OUTPUT_DIR}/")
    print("   è¯·è¿è¡Œ Plot_Exp0_3.py ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ã€‚")